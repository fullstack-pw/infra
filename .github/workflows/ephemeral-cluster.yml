name: Ephemeral Cluster Management

on:
  repository_dispatch:
    types: [create_ephemeral_cluster, delete_ephemeral_cluster]

env:
  VAULT_ADDR: https://vault.fullstack.pw
jobs:
  create-cluster:
    if: github.event_name == 'repository_dispatch' && github.event.action == 'create_ephemeral_cluster'
    runs-on: self-hosted
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check IP pool capacity
        id: check-capacity
        run: |
          AVAILABLE=$(./clusters/scripts/ip_pool_manager.sh check-capacity)
          CAPACITY_RESULT=$?
          echo "Capacity available: $AVAILABLE slots free"
          if [ "$CAPACITY_RESULT" -ne 0 ]; then
            echo "::error::IP pool exhausted. All 5 ephemeral cluster slots are in use (10 IPs / 2 per cluster). Please close old PRs or wait for auto-cleanup."
            exit 1
          fi

      - name: Allocate IP from pool
        id: allocate
        run: |
          CLUSTER_NAME="pr-${{ github.event.client_payload.repository }}-${{ github.event.client_payload.pr_number }}"
          CLUSTER_IP=$(./clusters/scripts/ip_pool_manager.sh allocate "$CLUSTER_NAME")

          # Calculate node IP (VIP + 1) - each cluster needs 2 IPs
          IP_LAST_OCTET=$(echo "$CLUSTER_IP" | cut -d'.' -f4)
          NODE_IP_LAST_OCTET=$((IP_LAST_OCTET + 1))
          NODE_IP="192.168.1.${NODE_IP_LAST_OCTET}"

          echo "ip=$CLUSTER_IP" >> $GITHUB_OUTPUT
          echo "node_ip=$NODE_IP" >> $GITHUB_OUTPUT
          echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
          echo "Allocated control plane VIP: $CLUSTER_IP for cluster: $CLUSTER_NAME"
          echo "Node IP: $NODE_IP"

      - name: Render Cluster API manifest
        run: |
          export CLUSTER_NAME="${{ steps.allocate.outputs.cluster_name }}"
          export CLUSTER_IP="${{ steps.allocate.outputs.ip }}"
          export NODE_IP="${{ steps.allocate.outputs.node_ip }}"
          export PR_NUMBER="${{ github.event.client_payload.pr_number }}"
          export REPOSITORY="${{ github.event.client_payload.repository }}"
          envsubst < ephemeral-clusters/cluster-api/k3s-cluster.yaml.tpl > /tmp/cluster.yaml
          cat /tmp/cluster.yaml

      - name: Create cluster via Cluster API
        run: |
          kubectl apply -f /tmp/cluster.yaml --context tools --kubeconfig /home/runner/.kube/config

      - name: Copy Proxmox credentials to namespace
        run: |
          CLUSTER_NAME="${{ steps.allocate.outputs.cluster_name }}"
          # Copy proxmox-credentials from another namespace (e.g., k3s-test)
          kubectl --context tools --kubeconfig /home/runner/.kube/config \
            get secret proxmox-credentials -n k3s-test -o json | \
            jq 'del(.metadata.namespace,.metadata.uid,.metadata.resourceVersion,.metadata.creationTimestamp,.metadata.ownerReferences,.metadata.finalizers)' | \
            kubectl --context tools --kubeconfig /home/runner/.kube/config apply -n "$CLUSTER_NAME" -f - || \
            echo "Warning: Could not copy proxmox-credentials (may already exist)"

      - name: Wait for cluster available
        run: |
          CLUSTER_NAME="${{ steps.allocate.outputs.cluster_name }}"
          echo "Waiting for cluster $CLUSTER_NAME to be available..."
          kubectl wait --for=condition=Available \
            cluster/$CLUSTER_NAME \
            -n $CLUSTER_NAME \
            --timeout=5m --context tools --kubeconfig /home/runner/.kube/config

      - name: Extract kubeconfig
        id: kubeconfig
        run: |
          CLUSTER_NAME="${{ steps.allocate.outputs.cluster_name }}"
          kubectl get secret ${CLUSTER_NAME}-kubeconfig \
            -n $CLUSTER_NAME \
            -o jsonpath='{.data.value}' --context tools --kubeconfig /home/runner/.kube/config| base64 -d > /tmp/kubeconfig
          echo "kubeconfig_path=/tmp/kubeconfig" >> $GITHUB_OUTPUT

      - name: Store kubeconfig in Vault
        run: |
          CLUSTER_NAME="${{ steps.allocate.outputs.cluster_name }}"
          vault kv put kv/ephemeral-clusters/${CLUSTER_NAME}/kubeconfig \
            value=@/tmp/kubeconfig

      - name: Install Phase 1 - Operators
        run: |
          kubectl apply -k ephemeral-clusters/phase1-operators/ --kubeconfig /tmp/kubeconfig

      - name: Install CloudNativePG via Helm (optional)
        if: github.event.client_payload.install_postgres == 'true'
        run: |
          helm repo add cloudnative-pg https://cloudnative-pg.github.io/charts || true
          helm repo update cloudnative-pg
          helm install cnpg cloudnative-pg/cloudnative-pg \
            --namespace cnpg-system \
            --create-namespace \
            --kubeconfig /tmp/kubeconfig \
            --wait --timeout=3m

      - name: Wait for operators ready
        run: |
          echo "Waiting for cert-manager..."
          kubectl wait --for=condition=Available deployment/cert-manager \
            -n cert-manager --timeout=3m --kubeconfig /tmp/kubeconfig

          echo "Waiting for external-dns..."
          kubectl wait --for=condition=Available deployment/external-dns-cloudflare \
            -n external-dns --timeout=3m --kubeconfig /tmp/kubeconfig

          echo "Waiting for external-secrets..."
          kubectl wait --for=condition=Available deployment/external-secrets \
            -n external-secrets --timeout=3m --kubeconfig /tmp/kubeconfig

          if [ "${{ github.event.client_payload.install_postgres }}" == "true" ]; then
            echo "Waiting for CloudNativePG..."
            kubectl wait --for=condition=Available deployment/cnpg-cloudnative-pg \
              -n cnpg-system --timeout=3m --kubeconfig /tmp/kubeconfig
          fi

      - name: Wait for CRDs established
        run: |
          CRDS="crd/certificates.cert-manager.io crd/clusterissuers.cert-manager.io crd/dnsendpoints.externaldns.k8s.io crd/secretstores.external-secrets.io crd/externalsecrets.external-secrets.io"

          if [ "${{ github.event.client_payload.install_postgres }}" == "true" ]; then
            CRDS="$CRDS crd/clusters.postgresql.cnpg.io crd/backups.postgresql.cnpg.io"
          fi

          kubectl wait --for condition=established --timeout=3m --kubeconfig /tmp/kubeconfig $CRDS

      - name: Install Phase 2 - Resources
        run: |
          export DNS_NAME="pr-${{ github.event.client_payload.pr_number }}-${{ github.event.client_payload.repository }}.ephemeral.fullstack.pw"
          export CLUSTER_IP="${{ steps.allocate.outputs.ip }}"
          export CLUSTER_NAME="${{ steps.allocate.outputs.cluster_name }}"

          # Apply ClusterIssuer (static)
          kubectl apply -f ephemeral-clusters/phase2-resources/clusterissuer.yaml --kubeconfig /tmp/kubeconfig

          # Apply templated resources
          for f in ephemeral-clusters/phase2-resources/*.tpl; do
            envsubst < "$f"
          done | kubectl apply --kubeconfig /tmp/kubeconfig -f -

      - name: Verify Phase 2 resources
        run: |
          echo "Verifying ClusterIssuer..."
          kubectl get clusterissuer letsencrypt-staging --kubeconfig /tmp/kubeconfig

          echo "Verifying ExternalSecret..."
          kubectl get externalsecret -n default --kubeconfig /tmp/kubeconfig

          if [ "${{ github.event.client_payload.install_postgres }}" == "true" ]; then
            echo "Verifying PostgreSQL Cluster..."
            kubectl get cluster.postgresql.cnpg.io postgres -n default --kubeconfig /tmp/kubeconfig
          fi

          echo "Phase 2 resources verified successfully"

      - name: Post cluster info comment
        if: success()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.PAT_TOKEN }}
          script: |
            const clusterName = '${{ steps.allocate.outputs.cluster_name }}';
            const clusterIP = '${{ steps.allocate.outputs.ip }}';
            const prNumber = parseInt('${{ github.event.client_payload.pr_number }}');
            const repo = '${{ github.event.client_payload.repository }}';
            const dnsName = 'pr-' + prNumber + '-' + repo + '.ephemeral.fullstack.pw';

            const body = '## Ephemeral Cluster Ready\n\n' +
              '**Cluster Name:** `' + clusterName + '`\n' +
              '**Cluster IP:** `' + clusterIP + '`\n' +
              '**DNS:** `' + dnsName + '`\n' +
              '**Status:** Ready for deployment\n\n' +
              'The ephemeral cluster is now available for testing. Your application will be deployed shortly.\n\n' +
              '---\n' +
              '*This cluster will auto-cleanup 1 hour after PR close or after 3 days maximum lifetime.*';

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: repo,
              issue_number: prNumber,
              body: body
            });

      - name: Cleanup on failure
        if: failure()
        run: |
          CLUSTER_NAME="${{ steps.allocate.outputs.cluster_name }}"

          # Delete cluster if it was created
          kubectl delete cluster $CLUSTER_NAME -n $CLUSTER_NAME --context tools --ignore-not-found=true --kubeconfig /home/runner/.kube/config

          # Release IP
          ./clusters/scripts/ip_pool_manager.sh release "$CLUSTER_NAME" || true

          # Remove kubeconfig from Vault
          vault kv delete kv/ephemeral-clusters/${CLUSTER_NAME}/kubeconfig || true

  delete-cluster:
    if: github.event_name == 'repository_dispatch' && github.event.action == 'delete_ephemeral_cluster'
    runs-on: self-hosted
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Delete Cluster API resources
        run: |
          CLUSTER_NAME="${{ github.event.client_payload.cluster_name }}"
          echo "Deleting cluster: $CLUSTER_NAME"

          kubectl delete cluster $CLUSTER_NAME -n $CLUSTER_NAME --context tools --ignore-not-found=true --kubeconfig /home/runner/.kube/config

          # Wait for cluster deletion
          kubectl wait --for=delete cluster/$CLUSTER_NAME -n $CLUSTER_NAME \
            --timeout=5m --context tools --kubeconfig /home/runner/.kube/config || true

          # Delete namespace
          kubectl delete namespace $CLUSTER_NAME --context tools --ignore-not-found=true --kubeconfig /home/runner/.kube/config

      - name: Release IP from pool
        run: |
          CLUSTER_NAME="${{ github.event.client_payload.cluster_name }}"
          ./clusters/scripts/ip_pool_manager.sh release "$CLUSTER_NAME"

      - name: Remove kubeconfig from Vault
        run: |
          CLUSTER_NAME="${{ github.event.client_payload.cluster_name }}"
          vault kv delete kv/ephemeral-clusters/${CLUSTER_NAME}/kubeconfig || true

      - name: Delete ArgoCD Application
        run: |
          CLUSTER_NAME="${{ github.event.client_payload.cluster_name }}"
          APP_NAME=$(echo "$CLUSTER_NAME" | sed 's/pr-//')
          kubectl delete application $APP_NAME -n argocd --context tools --ignore-not-found=true --kubeconfig /home/runner/.kube/config

      - name: Post cleanup confirmation
        if: success()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.PAT_TOKEN }}
          script: |
            const clusterName = '${{ github.event.client_payload.cluster_name }}';
            const prNumber = parseInt('${{ github.event.client_payload.pr_number }}');
            const repo = '${{ github.event.client_payload.repository }}';

            const body = '## Ephemeral Cluster Deleted\n\n' +
              '**Cluster Name:** `' + clusterName + '`\n' +
              '**Status:** Cleaned up\n\n' +
              'Resources released:\n' +
              '- VM deleted from Proxmox\n' +
              '- IP returned to pool\n' +
              '- Kubeconfig removed from Vault\n' +
              '- DNS records removed\n\n' +
              '---\n' +
              '*Ephemeral cluster resources have been fully cleaned up.*';

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: repo,
              issue_number: prNumber,
              body: body
            });
